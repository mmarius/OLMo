{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "\n",
    "from olmo.data import build_memmap_dataset\n",
    "from olmo.config import TrainConfig\n",
    "from olmo.tokenizer import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"/home/mila/m/marius.mosbach/projects/olmo/configs/slim_pajama/OLMo-59M.yaml\"\n",
    "training_config = TrainConfig.load(config_path)\n",
    "seq_length = training_config.model.max_sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the tokenizer\n",
    "tokenizer_name = training_config.tokenizer.identifier\n",
    "print(\"Loading tokenizer:\", tokenizer_name)\n",
    "tokenizer = Tokenizer.from_file(tokenizer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a memmap dataset from the training data\n",
    "dataset = build_memmap_dataset(training_config, training_config.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Build a memmap dataset from the validation data\n",
    "# validation_datasets = []\n",
    "# for evaluator in training_config.evaluators:\n",
    "#     if evaluator.type == \"lm\":\n",
    "#         evaluator.paths = None\n",
    "#         dataset = build_memmap_dataset(training_config, evaluator.data)\n",
    "#         validation_datasets.append(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = validation_datasets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of tokens in the dataset\n",
    "num_tokens = len(dataset) * training_config.model.max_sequence_length\n",
    "num_tokens_billions = num_tokens / 1e9\n",
    "print(f\"Number of tokens in the dataset: {num_tokens_billions:.2f}B\")\n",
    "num_tokens_millions = num_tokens / 1e6\n",
    "print(f\"Number of tokens in the dataset: {num_tokens_millions:.2f}M\")\n",
    "num_tokens_thousands = num_tokens / 1e3\n",
    "print(f\"Number of tokens in the dataset: {num_tokens_thousands:.2f}k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute number of steps based on total number of tokens\n",
    "num_tokens = num_tokens\n",
    "num_tokens_billions = num_tokens / 1e9\n",
    "print(f\"Number of tokens in the dataset: {num_tokens_billions:.2f}B\")\n",
    "num_tokens_millions = num_tokens / 1e6\n",
    "print(f\"Number of tokens in the dataset: {num_tokens_millions:.2f}M\")\n",
    "\n",
    "max_sequence_length = training_config.model.max_sequence_length\n",
    "# max_sequence_length = 512\n",
    "batch_size = training_config.global_train_batch_size\n",
    "# batch_size = 200\n",
    "\n",
    "num_chunks = num_tokens // max_sequence_length # number of chunks of sequence_length tokens\n",
    "num_steps = num_chunks // batch_size # how many batches we need to iterate over the chunks\n",
    "print(f\"Number of steps: {num_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load an example from the dataset\n",
    "example = dataset[0]\n",
    "print(example)\n",
    "input_ids = example[\"input_ids\"]\n",
    "print(input_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for example in dataset:\n",
    "    print(example.keys())\n",
    "    print(example[\"input_ids\"][:10])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decode the input ids\n",
    "example_text = tokenizer.decode(list(input_ids))\n",
    "print(example_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decode a dolma dataset\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/dolma2-tokenizer\")\n",
    "path = '/network/scratch/m/marius.mosbach/olmo/training_data/olmo2/slim_pajama/part-0-00000.npy'\n",
    "\n",
    "size = os.path.getsize(path)\n",
    "data = np.memmap(path, dtype='uint32', mode='r', shape=(size // 4,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(data[:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load csv.gz from disk\n",
    "path = \"/network/scratch/m/marius.mosbach/olmo/training_data/olmo2/slim_pajama/part-0-00000.csv.gz\"\n",
    "columns = [\"start\", \"end\", \"unkown\", \"path\", \"id\"]\n",
    "df = pd.read_csv(path, compression='gzip', names=columns)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"id\"] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(data[233290:234051]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "olmo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
